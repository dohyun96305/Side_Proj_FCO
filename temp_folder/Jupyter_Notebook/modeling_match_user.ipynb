{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import datetime \n",
    "import logging\n",
    "logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys \n",
    "sys.path.append('C:/Users/dohyu/Desktop/Github/side_proj_fifa')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rc('font', family = 'Malgun Gothic') # 한글 설정 (맑은 고딕)\n",
    "mpl.rc('axes', unicode_minus = False) # 음수 부호 깨지는거 설정\n",
    "\n",
    "import seaborn as sns \n",
    "import scipy.stats as stats\n",
    "\n",
    "import xgboost as xgb\n",
    "import sqlalchemy\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_conn(_username, _password, _host, _port, _database) : \n",
    "    db_engine = sqlalchemy.engine.URL.create(\n",
    "        drivername = \"mysql+pymysql\",\n",
    "        username = _username,\n",
    "        password = _password,\n",
    "        host = _host,\n",
    "        port = _port,\n",
    "        database = _database,\n",
    "    )\n",
    "\n",
    "    return create_engine(db_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db_conn('root', 'Dhyoon96!', '127.0.0.1', 3306, 'side_proj_fco')\n",
    "df_query = 'SELECT * FROM match_user'\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    df1 = pd.read_sql(df_query, con = connection)\n",
    "\n",
    "df1 = df1[df1['match_endtype'] == 0].reset_index(drop = True) # 정상 종료된 데이터만 사용\n",
    "\n",
    "# For rate columns (Variables that have 'try' and 'suc' columns)\n",
    "df1['match_total_pass_rate'] = (df1['match_total_pass_suc'] / df1['match_total_pass_try']).replace(np.nan, 0)\n",
    "\n",
    "df1['match_total_pass_short_rate'] = (df1['match_total_pass_short_suc'] / df1['match_total_pass_short_try']).replace(np.nan, 0)\n",
    "df1['match_total_pass_long_rate'] = (df1['match_total_pass_long_suc'] / df1['match_total_pass_long_try']).replace(np.nan, 0)\n",
    "df1['match_total_pass_through_rate'] = (df1['match_total_pass_through_suc'] / df1['match_total_pass_through_try']).replace(np.nan, 0)\n",
    "\n",
    "df1['match_total_shoot_inpenalty_rate'] = (df1['match_total_shoot_inpenalty_suc'] / df1['match_total_shoot_inpenalty_try']).replace(np.nan, 0)\n",
    "df1['match_total_shoot_outpenalty_rate'] = (df1['match_total_shoot_outpenalty_suc'] / df1['match_total_shoot_outpenalty_try']).replace(np.nan, 0)\n",
    "\n",
    "# For fail columns (Variables that have 'try' and 'suc' columns)\n",
    "df1['match_total_pass_fail'] = df1['match_total_pass_try'] - df1['match_total_pass_suc']\n",
    "\n",
    "df1['match_total_pass_short_fail'] = df1['match_total_pass_short_try'] - df1['match_total_pass_short_suc']\n",
    "df1['match_total_pass_long_fail'] = df1['match_total_pass_long_try'] - df1['match_total_pass_long_suc']\n",
    "df1['match_total_pass_through_fail'] = df1['match_total_pass_through_try'] - df1['match_total_pass_through_suc']\n",
    "\n",
    "df1['match_total_shoot_inpenalty_fail'] = df1['match_total_shoot_inpenalty_try'] - df1['match_total_shoot_inpenalty_suc']\n",
    "df1['match_total_shoot_outpenalty_fail'] = df1['match_total_shoot_outpenalty_try'] - df1['match_total_shoot_outpenalty_suc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_col = ['match_total_pass_rate',\n",
    "            'match_total_pass_short_rate', \n",
    "            'match_total_pass_long_rate', \n",
    "            'match_total_pass_through_rate', \n",
    "            'match_total_shoot_inpenalty_rate', \n",
    "            'match_total_shoot_outpenalty_rate']\n",
    "\n",
    "try_col = ['match_total_pass_try',                 \n",
    "           'match_total_pass_short_try',\n",
    "           'match_total_pass_long_try',\n",
    "           'match_total_pass_through_try',\n",
    "           'match_total_shoot_outpenalty_try',\n",
    "           'match_total_shoot_inpenalty_try']     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rfc = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "kf_rfc = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "param_grid_rfc = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "grid_search_rfc = GridSearchCV(estimator = model_rfc, \n",
    "                           param_grid = param_grid_rfc, \n",
    "                           cv = kf_rfc, \n",
    "                           n_jobs = -1, \n",
    "                           verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(_df, _col) : \n",
    "    df_temp = _df[_col]\n",
    "    df_temp = df_temp[((_df['match_result'] == '승') | (df_temp['match_result'] == '패'))].reset_index(drop = True)\n",
    "\n",
    "    df_temp['match_result'] = df_temp['match_result'].map({'승': 1, '패': 0})\n",
    "\n",
    "    X = df_temp.drop('match_result', axis = 1)\n",
    "    y = df_temp['match_result']\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, \n",
    "                                                      test_size = 0.2, \n",
    "                                                      random_state = 42, \n",
    "                                                      stratify = y)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_temp_scaled = scaler.fit_transform(X_temp)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    X_temp_scaled_gpu = cp.array(X_temp_scaled)\n",
    "    X_test_scaled_gpu = cp.array(X_test_scaled)\n",
    "    y_temp_gpu = cp.array(y_temp)\n",
    "    y_test_gpu = cp.array(y_test)\n",
    "\n",
    "    print('Model Random Forest Classifier with Column:', _col)\n",
    "\n",
    "    grid_search_rfc.fit(X_temp_scaled, y_temp)\n",
    "\n",
    "    best_model_rfc = grid_search_rfc.best_estimator_\n",
    "\n",
    "    y_test_pred_rfc = best_model_rfc.predict(X_test_scaled)\n",
    "    y_test_pred_proba_rfc = best_model_rfc.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    test_result_rfc = pd.DataFrame({\n",
    "        'Actual': y_test.values,\n",
    "        'Predicted': y_test_pred_rfc\n",
    "    })\n",
    "\n",
    "    cm_rfc = confusion_matrix(y_test, y_test_pred_rfc)\n",
    "    \n",
    "    fpr_rfc, tpr_rfc, thresholds_rfc = roc_curve(y_test, y_test_pred_proba_rfc)\n",
    "    roc_auc_score_rfc = roc_auc_score(y_test, y_test_pred_proba_rfc)\n",
    "\n",
    "    print(\"\\nTest Accuracy:\", accuracy_score(y_test, y_test_pred_rfc))\n",
    "    print(\"\\nClassification Report on Test Set_RFC:\\n\", classification_report(y_test, y_test_pred_rfc))\n",
    "\n",
    "    print('------------------------------------------------------------------------------------------------')\n",
    "    print('------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    feature_importance_avg_rfc = best_model_rfc.feature_importances_\n",
    "    sorted_idx_rfc = feature_importance_avg_rfc.argsort()\n",
    "\n",
    "    plt.figure(figsize = (12, 4))\n",
    "    plt.barh(X.columns[sorted_idx_rfc], feature_importance_avg_rfc[sorted_idx_rfc])\n",
    "    plt.xlabel(\"Average Feature Importance (Random Forest Classifier)\")\n",
    "    plt.title(\"Feature Importance Across K-Folds_RFC\")\n",
    "\n",
    "    plt.tick_params(axis = 'y', labelsize = 5)  \n",
    "    plt.tick_params(axis = 'x') \n",
    "\n",
    "    for i, v in enumerate(feature_importance_avg_rfc[sorted_idx_rfc]):\n",
    "        plt.text(v + 0.001, i, str(round(v, 2)), color = 'black', va = 'center', fontsize = 'small')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize = (12, 6))\n",
    "    \n",
    "    disp_rfc = ConfusionMatrixDisplay(confusion_matrix = cm_rfc, display_labels = best_model_rfc.classes_)\n",
    "    disp_rfc.plot(cmap = 'Blues', ax = axes[0], values_format = 'd', colorbar = False)    \n",
    "    axes[0].set_title(\"Confusion Matrix for Test Data_RFC\")\n",
    "\n",
    "    axes[1].plot(fpr_rfc, tpr_rfc, \n",
    "                 label = 'Random Forest Classifier (AUC = %0.2f)' % roc_auc_score_rfc)\n",
    "    axes[1].plot([0, 1], [0, 1], linestyle = '--', color = 'gray')\n",
    "    axes[1].set_xlabel('False Positive Rate')\n",
    "    axes[1].set_ylabel('True Positive Rate')\n",
    "    axes[1].set_title('Receiver Operating Characteristic Curve')\n",
    "    axes[1].legend(loc = 'lower right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with try_col, rate_col\n",
    "df_col_temp = ['match_result'] + try_col + rate_col\n",
    "\n",
    "random_forest_classifier(df1, df_col_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classifier with K-fold, GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col_temp = ['match_result'] + rate_col + try_col\n",
    "\n",
    "df2 = df1[df_col_temp]\n",
    "df2 = df2[((df2['match_result'] == '승') | (df2['match_result'] == '패'))].reset_index(drop = True)\n",
    "\n",
    "df2['match_result'] = df2['match_result'].map({'승': 1, '패': 0})\n",
    "\n",
    "X = df2.drop('match_result', axis = 1)\n",
    "y = df2['match_result']\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, \n",
    "                                                  test_size = 0.2, \n",
    "                                                  random_state = 42, \n",
    "                                                  stratify = y)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_temp_scaled = scaler.fit_transform(X_temp)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_temp_scaled_gpu = cp.array(X_temp_scaled)\n",
    "X_test_scaled_gpu = cp.array(X_test_scaled)\n",
    "y_temp_gpu = cp.array(y_temp)\n",
    "y_test_gpu = cp.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBClassifier(\n",
    "    tree_method = 'gpu_hist',  \n",
    "    eval_metric = \"logloss\",\n",
    "    use_label_encoder = False,  \n",
    "    random_state = 42)\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9]}\n",
    "\n",
    "kf_xgb = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "\n",
    "grid_search_xgb = GridSearchCV(estimator = model_xgb, \n",
    "                               param_grid = param_grid_xgb, \n",
    "                               cv = kf_xgb, \n",
    "                               verbose = 1, \n",
    "                               n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_xgb.fit(X_temp_scaled_gpu.get(), y_temp_gpu.get())\n",
    "\n",
    "print(\"Best parameters found: \", grid_search_xgb.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search_xgb.best_score_)\n",
    "\n",
    "best_model_xgb = grid_search_xgb.best_estimator_\n",
    "\n",
    "y_test_pred_xgb = best_model_xgb.predict(X_test_scaled_gpu.get())\n",
    "\n",
    "print(\"\\nTest Accuracy:\", accuracy_score(y_test_gpu.get(), y_test_pred_xgb))\n",
    "print(\"\\nClassification Report on Test Set:\\n\", classification_report(y_test_gpu.get(), y_test_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_avg_xgb = best_model_xgb.feature_importances_\n",
    "sorted_idx_xgb = feature_importance_avg_xgb.argsort()\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.barh(X.columns[sorted_idx_xgb], feature_importance_avg_xgb[sorted_idx_xgb])\n",
    "plt.xlabel(\"Average Feature Importance (XGBoost)\")\n",
    "plt.title(\"Feature Importance Across K-Folds_XGB\")\n",
    "\n",
    "plt.tick_params(axis = 'y', labelsize = 5)  \n",
    "plt.tick_params(axis = 'x')  \n",
    "\n",
    "for i, v in enumerate(feature_importance_avg_xgb[sorted_idx_xgb]):\n",
    "    plt.text(v + 0.001, i, str(round(v, 2)), color = 'black', va = 'center', fontsize = 'small')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_xgb = pd.DataFrame({\n",
    "    'Actual': y_test.values,\n",
    "    'Predicted': y_test_pred_xgb\n",
    "})\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, y_test_pred_xgb)\n",
    "\n",
    "disp_rc = ConfusionMatrixDisplay(confusion_matrix = cm_xgb, display_labels = best_model_xgb.classes_)\n",
    "disp_rc.plot(cmap = 'Blues', values_format = 'd')\n",
    "plt.title(\"Confusion Matrix for Test Data_XGB\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "side_proj_fco",
   "language": "python",
   "name": "project2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
